{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os.path\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Flatten, ZeroPadding2D, UpSampling2D\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import pandas as pd \n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from scipy.signal import savgol_filter\n",
    "import math\n",
    "import time\n",
    "import statistics\n",
    "\n",
    "pathname = r\"D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\"\n",
    "datadirname = \"data_new\"\n",
    "testdirname = \"test\"\n",
    "validdirname = \"valid\"\n",
    "modeldirname = \"model\"\n",
    "datacsvname = \"data.csv\"\n",
    "modeljsonname=\"model-regr.json\"\n",
    "modelweightname=\"model-regr.h5\"\n",
    "dim = (50,50) \n",
    "actionstonum = {\"RIGHT\": 0,\n",
    "           \"LEFT\": 1,\n",
    "           \"SPACE\" : 2,\n",
    "          }\n",
    "numtoactions = {0: \"RIGHT\",\n",
    "           1: \"LEFT\",\n",
    "           2: \"SPACE\",\n",
    "          }\n",
    "scores = []\n",
    "overallscores = []\n",
    "manual = False\n",
    "\n",
    "\n",
    "def create_q_model():\n",
    "        # Network defined by the Deepmind paper\n",
    "        inputs = layers.Input(shape=(dim[0], dim[1], 3,))\n",
    "\n",
    "        # Convolutions on the frames on the screen\n",
    "        layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "        layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "        layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "\n",
    "        layer4 = layers.Flatten()(layer3)\n",
    "\n",
    "        layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "        action = layers.Dense(3, activation=\"linear\")(layer5)\n",
    "\n",
    "        return keras.Model(inputs=inputs, outputs=action)\n",
    "\n",
    "def run_game(learning_rate = 1.5e-06, epochs = 5, benchmin = 68.0):\n",
    "    lr = [learning_rate for i in range(epochs)]\n",
    "\n",
    "    iterations = len(lr)\n",
    "    benches = []\n",
    "    qms = []\n",
    "    qps = []\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(iterations):\n",
    "        print(f\"{i}: learning rate: {lr[i]}\")\n",
    "        print(benchmin)\n",
    "        k = 30\n",
    "        game = Game(500,500)\n",
    "        game.load_replay_memory()\n",
    "        for j in range(k):\n",
    "            game.initialize()\n",
    "            game.run(j)\n",
    "        bench, qm, qp = game.print_benchmark()\n",
    "        benches.append(bench)\n",
    "        qms.append(qm)\n",
    "        qps.append(qp)\n",
    "        game.save_replay_memory()\n",
    "        game.save_checkpoint(f\"model-regr_{i}_{lr[i]:.9f}_{bench:.2f}.h5\")\n",
    "        if bench < benchmin:\n",
    "            benchmin = bench\n",
    "            game.save_checkpoint()\n",
    "        else:\n",
    "            counter += 1\n",
    "        if counter == 3:\n",
    "            counter = 0\n",
    "            lr = [i*0.5 for i in lr] \n",
    "\n",
    "    return benches, qms, qps\n",
    "\n",
    "model = create_q_model()\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(pathname, modeldirname,modeljsonname), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(os.path.join(pathname, modeldirname,modelweightname))\n",
    "\n",
    "\n",
    "class Game:\n",
    "    screen = None\n",
    "    \n",
    "    lost = False\n",
    "    done = False\n",
    "\n",
    "    def __init__(self, width, height, lr=1e-3, checkpointparname=\"model-regr.h5\"):\n",
    "        \n",
    "        self.currentAction = \"\"\n",
    "                       \n",
    "        self.shufflelist = []\n",
    "        \n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.screen = pygame.display.set_mode((int(width), int(height)))\n",
    "        self.screen.fill([255,0,0])\n",
    "\n",
    "        self.imgresh1 = None\n",
    "        self.imgresh2 = None\n",
    "\n",
    "        self.reward = 0\n",
    "        self.MAXREWARD = 1\n",
    "        self.HITREWARD = 1\n",
    "        self.PENALTY = -2\n",
    "        self.FIREPENALTY = -1\n",
    "        self.MOVEPENALTY = 0\n",
    "        self.NOTFIREPENALTY = -2\n",
    "        \n",
    "        self.BATCHSIZE = 19\n",
    "        self.DISCOUNT = 0.99\n",
    "        self.ALPHA = 0.3\n",
    "        \n",
    "        if manual == True:\n",
    "            self.EPSILON = 0.999\n",
    "        else:\n",
    "            self.EPSILON = 0.3\n",
    "        \n",
    "        self.REPLAYSIZE = 40_000\n",
    "        self.overall_score = 0\n",
    "        self.overall_numbatches = 0\n",
    "        self.overall_accumulatedstates = np.array([0.0,0.0,0.0,0.0])\n",
    "        \n",
    "        self.scores = []\n",
    "        \n",
    "        \n",
    "        self.path = os.path.join(pathname, datadirname)\n",
    "        self.modelpath =  os.path.join(pathname, modeldirname)\n",
    "        \n",
    "        self.filename = \"data.csv\"\n",
    "        \n",
    "        self.model = create_q_model()\n",
    "        self.model_target = create_q_model()\n",
    "\n",
    "        self.learningrate = lr\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=self.learningrate, clipnorm=1.0)\n",
    "        self.loss_function = keras.losses.Huber()\n",
    "\n",
    "        self.checkpointname = os.path.join(pathname, modeldirname,checkpointparname)\n",
    "        print(f\"loading checkpoint: {self.checkpointname}\")\n",
    "        self.model_target.load_weights(self.checkpointname)\n",
    "        \n",
    "        self.overall_scores=[]\n",
    "        self.checkpoint_counter=0\n",
    "        \n",
    "        self.debugcounter = 0\n",
    "\n",
    "        \n",
    "    def initialize(self):\n",
    "        pygame.init()\n",
    "        self.aliens = []\n",
    "        self.rockets = []\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.hero = Hero(self, self.width / 2, self.height - 20)\n",
    "        self.firstFire = True\n",
    "        \n",
    "        #Generator\n",
    "        margin = 30  # mezera od okraju obrazovky\n",
    "        width = 50  # mezera mezi alieny\n",
    "        self.screen = pygame.display.set_mode((int(self.width), int(self.height)))\n",
    "        for x in range(margin, self.width - margin, width):\n",
    "            for y in range(margin, int(self.height / 2), width):\n",
    "                if(random.randint(0,1)==1):\n",
    "                    self.aliens.append(Alien(self, x, y))\n",
    "                    \n",
    "        self.rocket = None\n",
    "        self.numbatches = 0\n",
    "\n",
    "    def run(self, i_index):\n",
    "        i = i_index + self.get_maxi() + 1\n",
    "        j = 0\n",
    "        while True:\n",
    "            img1 = np.frombuffer(pygame.image.tostring(self.screen, \"RGB\"), dtype=np.uint8)\n",
    "            self.imgresh1 = np.reshape(img1,(self.width,self.height, 3))\n",
    "            self.imgresh1 = cv2.resize(self.imgresh1, dim, interpolation = cv2.INTER_NEAREST )\n",
    "            self.imgresh1 = cv2.cvtColor(self.imgresh1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            current_state = np.array(self.imgresh1, dtype=np.float32)/255.0\n",
    "            state_tensor = tf.convert_to_tensor(current_state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = self.model(state_tensor, training=False)\n",
    "            theaction = tf.argmax(action_probs[0]).numpy()            \n",
    "\n",
    "            #win\n",
    "            if len(self.aliens) == 0:\n",
    "                pygame.display.flip()                         \n",
    "                pygame.quit()\n",
    "                return\n",
    "\n",
    "            pressed = pygame.key.get_pressed()\n",
    "            if pressed[pygame.K_LEFT]:\n",
    "                self.currentAction = \"LEFT\"\n",
    "                \n",
    "            elif pressed[pygame.K_RIGHT]:\n",
    "                self.currentAction = \"RIGHT\"\n",
    "                \n",
    "            elif pressed[pygame.K_q]:\n",
    "                pygame.display.flip()                         \n",
    "                pygame.quit()\n",
    "                return\n",
    "\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.KEYDOWN and event.key == pygame.K_SPACE and not self.lost:\n",
    "                    self.currentAction = \"SPACE\"\n",
    "                \n",
    "            if manual != True:\n",
    "                if np.random.random() > self.EPSILON:\n",
    "                    self.currentAction = numtoactions[theaction]\n",
    "                else:\n",
    "                    self.currentAction = numtoactions[random.randint(0,2)]\n",
    "\n",
    "                \n",
    "            if self.currentAction == \"RIGHT\":\n",
    "                self.hero.x += 2 if self.hero.x < self.width - 20 else 0  # prava hranice\n",
    "                self.reward = self.MOVEPENALTY\n",
    "                \n",
    "            if self.currentAction == \"LEFT\":\n",
    "                self.hero.x -= 2 if self.hero.x > 20 else 0  # leva hranice plochy\n",
    "                self.reward = self.MOVEPENALTY\n",
    "            \n",
    "            if len(self.rockets) == 0:\n",
    "                self.firstFire = True\n",
    "            \n",
    "            if self.currentAction == \"SPACE\":\n",
    "                if self.firstFire or self.rockets[-1].y + self.rockets[-1].size < self.hero.y:\n",
    "                    self.rockets.append(Rocket(self, self.hero.x, self.hero.y))\n",
    "                    if self.checkHit(self.rockets[-1]):\n",
    "                        self.reward = self.HITREWARD\n",
    "                    else:\n",
    "                        self.reward = self.FIREPENALTY\n",
    "                else:\n",
    "                    reward = self.NOTFIREPENALTY\n",
    "                self.firstFire = False\n",
    "            \n",
    "            if manual == True:\n",
    "                self.currentAction = \"LEFT\"\n",
    "\n",
    "            pygame.display.flip()\n",
    "            self.clock.tick(60)\n",
    "            self.screen.fill((255, 0, 0))\n",
    "\n",
    "            for alien in self.aliens:\n",
    "                alien.draw()\n",
    "                alien.checkCollision(self)\n",
    "                if (alien.y > (self.height-60)):\n",
    "                    self.reward = self.PENALTY\n",
    "                    self.train(i,j, True)\n",
    "                    self.scores.append(len(self.aliens))\n",
    "                    pygame.display.flip()                         \n",
    "                    pygame.quit()\n",
    "                    return\n",
    "                    \n",
    "            for rocket in self.rockets:\n",
    "                rocket.draw()\n",
    "\n",
    "            if not self.lost: self.hero.draw()\n",
    "                \n",
    "            img2 = np.frombuffer(pygame.image.tostring(self.screen, \"RGB\"), dtype=np.uint8)\n",
    "            self.imgresh2 = np.reshape(img2,(self.width,self.height, 3))\n",
    "            self.imgresh2 = cv2.resize(img2, dim, interpolation = cv2.INTER_NEAREST )\n",
    "            self.imgresh2 = cv2.cvtColor(self.imgresh2, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            if j > 0:\n",
    "                if self.reward >= 1:\n",
    "                    self.train(i,j, False)\n",
    "                elif j%4 == 0:\n",
    "                    self.train(i,j, False)\n",
    "\n",
    "            j+=1\n",
    "\n",
    "    def checkHit(self, rocket):\n",
    "        for alien in self.aliens:\n",
    "            if (rocket.x < alien.x + alien.size and \n",
    "                    rocket.x + rocket.size > alien.x and\n",
    "                    not alien.will_be_hit):\n",
    "                alien.will_be_hit = True\n",
    "                return True\n",
    "            \n",
    "            \n",
    "    def write(self, i, j): \n",
    "\n",
    "        cv2.imwrite(os.path.join(self.path,\"current_{}_{}.png\".format(i,j)), self.imgresh1)\n",
    "        cv2.imwrite(os.path.join(self.path,\"next_{}_{}.png\".format(i,j)), self.imgresh2)\n",
    "\n",
    "    def train(self, i, j, term):\n",
    "        \n",
    "        # https://pythonprogramming.net/training-deep-q-learning-dqn-reinforcement-learning-python-tutorial/\n",
    "        \n",
    "        currentstate = \"current_{}_{}.png\".format(i,j)\n",
    "\n",
    "        nextstate = \"next_{}_{}.png\".format(i,j)      \n",
    "        \n",
    "        batch, files = self.pop_batch(self.BATCHSIZE)\n",
    "        \n",
    "        assert(self.imgresh1.shape == (dim[0], dim[1],3))\n",
    "        assert(self.imgresh2.shape == (dim[0], dim[1],3))\n",
    "        \n",
    "        batch.append([self.imgresh1, actionstonum[self.currentAction], self.reward, self.imgresh2, term])\n",
    "        files.append((\"current_{}_{}.png\".format(i,j), \"next_{}_{}.png\".format(i,j)))\n",
    "        \n",
    "        self.write(i,j)\n",
    "         \n",
    "        self.backprop(batch)\n",
    "        \n",
    "        self.numbatches += 1\n",
    "            \n",
    "        self.push_batch(batch, files)   \n",
    "  \n",
    "        return    \n",
    "\n",
    "    def backprop(self, batch):\n",
    "\n",
    "        rewards_sample = [batch[i][2] for i in range(len(batch))]\n",
    "        action_sample = [batch[i][1] for i in range(len(batch))]\n",
    "      \n",
    "        done_sample = tf.convert_to_tensor([float(batch[i][4]) for i in range(len(batch))])\n",
    "\n",
    "        X =  self.get_X(batch, 0)\n",
    "        Xf = self.get_X(batch, 3)\n",
    "        future_rewards = self.model_target.predict(Xf)\n",
    "\n",
    "        updated_q_values = rewards_sample + 0.99 * tf.reduce_max(future_rewards, axis=1)\n",
    "        updated_q_values = updated_q_values * (1 - done_sample) - done_sample*abs(self.PENALTY)\n",
    "\n",
    "    \n",
    "        masks = tf.one_hot(action_sample, 3)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Train the model on the states and updated Q-values\n",
    "            q_values = self.model(X)\n",
    "\n",
    "            # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "            q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "            # Calculate loss between new Q-value and old Q-value\n",
    "            loss = self.loss_function(updated_q_values, q_action)\n",
    "            \n",
    "            #if self.debugcounter % 20 == 0:\n",
    "            #    print(self.debugcounter)\n",
    "            #    print(updated_q_values)\n",
    "            #    print(rewards_sample)\n",
    "            #    print(q_action)\n",
    "            #self.debugcounter += 1\n",
    "\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        \n",
    "    def get_X(self, batch, state):\n",
    "        \n",
    "        assert state == 0 or state == 3 # 0 is currentstate, 3 is future state\n",
    "        \n",
    "        X = [item[state] for item in batch]\n",
    "\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        \n",
    "        #for im in X:\n",
    "        #    assert im.min() == 0.0\n",
    "        #    assert im.max() == 255.0 or im.max() == 0.0\n",
    "        \n",
    "        X /= 255.0\n",
    "        \n",
    "        return X\n",
    "\n",
    "    def pop_batch(self, batchsize):\n",
    "       \n",
    "        batch = []\n",
    "        files = []\n",
    "    \n",
    "        for i in range(batchsize):\n",
    "            \n",
    "            item = self.shufflelist.pop(0)\n",
    "            \n",
    "            assert os.path.isfile(os.path.join(self.path, item[0]))\n",
    "            assert os.path.isfile(os.path.join(self.path, item[3]))\n",
    "            img1 = cv2.imread(os.path.join(self.path, item[0]),cv2.IMREAD_COLOR )\n",
    "            img2 = cv2.imread(os.path.join(self.path, item[3]),cv2.IMREAD_COLOR )\n",
    "\n",
    "            batch.append([img1, item[1], item[2], img2, item[4]])\n",
    "            files.append((item[0],item[3]))\n",
    "\n",
    "        return batch, files\n",
    "\n",
    "    def push_batch(self, batch, files):\n",
    "       \n",
    "        for index,item in enumerate(batch):\n",
    "            assert item[0].shape == (dim[0], dim[1], 3)\n",
    "            assert (item[1] < len(numtoactions) and item[1] >= 0)\n",
    "            assert isinstance(item[2],int) or isinstance(item[2],float)\n",
    "            assert item[3].shape == (dim[0], dim[1], 3)\n",
    "            assert os.path.isfile(os.path.join(self.path, files[index][0]))\n",
    "            assert os.path.isfile(os.path.join(self.path, files[index][1]))\n",
    "            \n",
    "            self.shufflelist.append([files[index][0], item[1], item[2], files[index][1], item[4]])\n",
    "    \n",
    "        return\n",
    "    \n",
    "    def get_maxi(self):\n",
    "        \n",
    "        maxi = 0\n",
    "        \n",
    "        for item in self.shufflelist:\n",
    "            curr = item[0]\n",
    "            s = re.findall(r'\\d+', curr)[0]\n",
    "            if int(s) > maxi:\n",
    "                maxi = int(s)\n",
    "        \n",
    "        return maxi\n",
    "    \n",
    "    def load_replay_memory(self):\n",
    "\n",
    "        f = open(os.path.join(os.path.join(self.path,datacsvname)), \"r\")\n",
    "        \n",
    "        df = pd.read_csv(f, index_col = 0) \n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "\n",
    "            currentpicname = row[\"currentstate\"]\n",
    "            action = actionstonum[row[\"action\"]]\n",
    "            reward = row[\"reward\"]\n",
    "            nextpicname = row[\"nextstate\"]\n",
    "            terminated = row[\"terminated\"]\n",
    "\n",
    "            \n",
    "            assert os.path.isfile(os.path.join(self.path,currentpicname)) == True\n",
    "            assert (action < 5 and action >= 0)\n",
    "            assert isinstance(reward,int) or isinstance(reward, float)\n",
    "            assert os.path.isfile(os.path.join(self.path,nextpicname)) == True\n",
    "            \n",
    "            self.shufflelist.append([currentpicname,action,reward,nextpicname, terminated])\n",
    "\n",
    "        random.shuffle(self.shufflelist)\n",
    "        \n",
    "        #print(self.shufflelist)\n",
    "\n",
    "        #print(f\"loading: size of replay memory {len(self.shufflelist)}\")\n",
    "        \n",
    "        f.close()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def save_replay_memory(self):\n",
    "        \n",
    "        assert os.path.isfile(os.path.join(self.path,datacsvname)) == True\n",
    "        \n",
    "        data = []\n",
    "        \n",
    "        if len(self.shufflelist) == 0:\n",
    "            return\n",
    "        \n",
    "        if len(self.shufflelist) > self.REPLAYSIZE:\n",
    "            \n",
    "            self.numbatches = len(self.shufflelist) - self.REPLAYSIZE\n",
    "            self.overall_numbatches += self.numbatches\n",
    "            \n",
    "            for i in range(len(self.shufflelist) - self.REPLAYSIZE):\n",
    "                item = self.shufflelist.pop(0)\n",
    "                assert os.path.isfile(os.path.join(self.path,item[0])) == True\n",
    "                assert os.path.isfile(os.path.join(self.path,item[3])) == True\n",
    "                os.remove(os.path.join(self.path,item[0]))\n",
    "                os.remove(os.path.join(self.path,item[3]))\n",
    "                \n",
    "        for (cs, act, rew, fs, term) in self.shufflelist:\n",
    "            \n",
    "            data.append({'currentstate': cs, 'action': numtoactions[act], 'reward': rew, 'nextstate': fs, 'terminated': term})\n",
    "            \n",
    "        df = pd.DataFrame(data) \n",
    "        \n",
    "        df.to_csv(os.path.join(self.path, self.filename)) \n",
    "        \n",
    "        #print(f\"saving: size of replay memory {len(self.shufflelist)}\")\n",
    "    \n",
    "        return\n",
    "    \n",
    "    def print_benchmark(self):\n",
    "\n",
    "        maxlist = []\n",
    "        penaltylist = []\n",
    "        averagestates = [0,0,0]\n",
    "        averagepenalty = [0,0,0]\n",
    "        pmerror = 0\n",
    "        pterror = 0\n",
    "\n",
    "        for (cs, act, rew, fs, term) in self.shufflelist:\n",
    "            if rew >= 1:\n",
    "                maxlist.append((cs,act,rew,fs,term))\n",
    "            if rew <= self.PENALTY:\n",
    "                penaltylist.append((cs,act,rew,fs,term))\n",
    "        print(f\"Number of maxrewards in shufflelist: {len(maxlist)}, perc: {100*len(maxlist)/len(self.shufflelist)}\")\n",
    "        print(f\"Number of terminations in shufflelist: {len(penaltylist)}, perc: {100*len(penaltylist)/len(self.shufflelist)}\")\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        print(\"Testing maxlist\")\n",
    "        for i in range(len(maxlist)):\n",
    "            img = cv2.imread(os.path.join(pathname, datadirname, maxlist[i][0]),cv2.IMREAD_COLOR )\n",
    "            states = self.model.predict(np.array([img])/255.0, batch_size=1, verbose=0)[0]\n",
    "            averagestates += states\n",
    "            if np.argmax(states) != maxlist[i][1]:\n",
    "                count += 1\n",
    "            pmerror = 100*count/len(maxlist)\n",
    "        print(f\"Number of predicted errors in maxlist: {count}, perc: {pmerror}\")\n",
    "        print(f\"Q Values for max: {[i/len(maxlist) for i in averagestates]}\")\n",
    "        print(\"score: \" + str(self.scores))\n",
    "        print(\"average score: \" + str(statistics.mean(self.scores)))\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        print(\"Testing penaltylist\") \n",
    "        for i in range(len(penaltylist)):\n",
    "            img = cv2.imread(os.path.join(pathname, datadirname, penaltylist[i][0]),cv2.IMREAD_COLOR )\n",
    "            states = self.model.predict(np.array([img])/255.0, batch_size=1, verbose=0)[0]\n",
    "            averagepenalty += states\n",
    "            if np.argmax(states) == penaltylist[i][1]:\n",
    "                count += 1\n",
    "            pterror = 100*count/len(penaltylist)\n",
    "        print(f\"Number of predicted terminations in penaltylist: {count}, perc: {pterror}\")\n",
    "        print(f\"Q Values for penalty: {[i/len(penaltylist) for i in averagepenalty]}\")\n",
    "        \n",
    "        return pmerror, [i/len(maxlist) for i in averagestates], [i/len(penaltylist) for i in averagepenalty]\n",
    "    \n",
    "    def save_checkpoint(self, checkpointparname=modelweightname):\n",
    "                                                                         \n",
    "        self.model_target.set_weights(self.model.get_weights())\n",
    "        print(f\"saving checkpoint: {os.path.join(pathname, modeldirname,checkpointparname)}\")\n",
    "        self.model_target.save_weights(os.path.join(pathname, modeldirname,checkpointparname) )\n",
    "            \n",
    "        return\n",
    "\n",
    "    def print_score(self):\n",
    "        print(f\" ----> TIME IS {datetime.now():%Y-%m-%d_%H-%M-%S}\")\n",
    "        print(f\" ----> SCORE is {self.score}\")\n",
    "        print(f\" ----> NUM OF BATCHES is {self.numbatches}\")\n",
    "        return self.score, self.numbatches\n",
    "    \n",
    "    def print_overall_score(self):\n",
    "        print(f\"--> TIME IS {datetime.now():%Y-%m-%d_%H-%M-%S}\")\n",
    "        print(f\"--> OVERALL SCORE is {self.overall_score}\")\n",
    "        print(f\"--> OVERALL NUM OF BATCHES is {self.overall_numbatches}\")\n",
    "        return self.overall_score, self.overall_numbatches     \n",
    "    \n",
    "\n",
    "\n",
    "class Alien:\n",
    "    def __init__(self, game, x, y):\n",
    "        self.x = x\n",
    "        self.game = game\n",
    "        self.y = y\n",
    "        self.size = 40\n",
    "        self.will_be_hit = False\n",
    "\n",
    "    def draw(self):\n",
    "        pygame.draw.rect(self.game.screen,(0, 0, 255),  # barva objektu\n",
    "                         pygame.Rect(self.x, self.y, self.size, self.size))\n",
    "        self.y += 0.6\n",
    "\n",
    "    def checkCollision(self, game):\n",
    "        for rocket in game.rockets:\n",
    "            if (rocket.x < self.x + self.size and\n",
    "                    rocket.x + rocket.size > self.x and\n",
    "                    rocket.y < self.y + self.size and\n",
    "                    rocket.y > self.y - self.size):\n",
    "                game.rockets.remove(rocket)\n",
    "                game.aliens.remove(self)\n",
    "\n",
    "\n",
    "class Hero:\n",
    "    def __init__(self, game, x, y):\n",
    "        self.x = x\n",
    "        self.game = game\n",
    "        self.y = y\n",
    "\n",
    "    def draw(self):\n",
    "        pygame.draw.rect(self.game.screen,\n",
    "                         (255, 255, 255),\n",
    "                         pygame.Rect(self.x, self.y, 40, 20))\n",
    "\n",
    "\n",
    "class Rocket:\n",
    "    def __init__(self, game, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.game = game\n",
    "        self.size = 15\n",
    "\n",
    "    def draw(self):\n",
    "        pygame.draw.rect(self.game.screen,(0, 255, 0),pygame.Rect(self.x, self.y, self.size, self.size))\n",
    "        self.y -= 3 \n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    " #   game = Game(500, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: learning rate: 1.5e-06\n",
      "60.0\n",
      "loading checkpoint: D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\\model\\model-regr.h5\n",
      "Number of maxrewards in shufflelist: 1740, perc: 20.921005170133462\n",
      "Number of terminations in shufflelist: 31, perc: 0.37273055188168813\n",
      "Testing maxlist\n",
      "Number of predicted errors in maxlist: 143, perc: 8.218390804597702\n",
      "Q Values for max: [0.092042694502007, 0.09980354133522373, 0.7646029183555437]\n",
      "score: [15, 18, 17, 13, 21, 14, 17, 14, 10, 14, 17, 15, 15, 14, 15, 15, 14, 11, 14, 19, 21, 10, 14, 12, 18, 14, 13, 17, 13, 17]\n",
      "average score: 15.033333333333333\n",
      "Testing penaltylist\n",
      "Number of predicted terminations in penaltylist: 11, perc: 35.483870967741936\n",
      "Q Values for penalty: [-0.27341900321264423, -0.11672645639027318, -0.4258064869190416]\n",
      "saving checkpoint: D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\\model\\model-regr_0_0.000001500_8.22.h5\n",
      "saving checkpoint: D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\\model\\model-regr.h5\n",
      "1: learning rate: 1.5e-06\n",
      "8.218390804597702\n",
      "loading checkpoint: D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\\model\\model-regr.h5\n",
      "Number of maxrewards in shufflelist: 2448, perc: 21.244467586566\n",
      "Number of terminations in shufflelist: 61, perc: 0.5293760305476004\n",
      "Testing maxlist\n",
      "Number of predicted errors in maxlist: 312, perc: 12.745098039215685\n",
      "Q Values for max: [0.087697933016351, 0.04951054652433027, 0.7534056453991475]\n",
      "score: [12, 17, 15, 16, 17, 11, 21, 15, 18, 16, 17, 7, 13, 11, 15, 11, 16, 12, 16, 16, 15, 12, 14, 18, 14, 13, 12, 11, 20, 9]\n",
      "average score: 14.333333333333334\n",
      "Testing penaltylist\n",
      "Number of predicted terminations in penaltylist: 12, perc: 19.672131147540984\n",
      "Q Values for penalty: [-0.13665290273054212, -0.10323377520029174, -0.22206496673284984]\n",
      "saving checkpoint: D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\\model\\model-regr_1_0.000001500_12.75.h5\n",
      "2: learning rate: 1.5e-06\n",
      "8.218390804597702\n",
      "loading checkpoint: D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\\model\\model-regr.h5\n",
      "Number of maxrewards in shufflelist: 3204, perc: 21.742671009771986\n",
      "Number of terminations in shufflelist: 91, perc: 0.6175352877307275\n",
      "Testing maxlist\n",
      "Number of predicted errors in maxlist: 250, perc: 7.802746566791511\n",
      "Q Values for max: [0.0452196037387487, 0.1202808783346608, 0.8238807808033443]\n",
      "score: [16, 23, 18, 15, 17, 16, 23, 9, 9, 18, 10, 12, 17, 14, 18, 15, 11, 7, 12, 10, 14, 16, 15, 16, 14, 14, 15, 17, 10, 15]\n",
      "average score: 14.533333333333333\n",
      "Testing penaltylist\n",
      "Number of predicted terminations in penaltylist: 29, perc: 31.86813186813187\n",
      "Q Values for penalty: [-0.2373453975542561, -0.17544257391121362, -0.5387901526231033]\n",
      "saving checkpoint: D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\\model\\model-regr_2_0.000001500_7.80.h5\n",
      "saving checkpoint: D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\\model\\model-regr.h5\n",
      "3: learning rate: 1.5e-06\n",
      "7.802746566791511\n",
      "loading checkpoint: D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\\model\\model-regr.h5\n",
      "Number of maxrewards in shufflelist: 3795, perc: 21.281965006729475\n",
      "Number of terminations in shufflelist: 121, perc: 0.6785554060116644\n",
      "Testing maxlist\n",
      "Number of predicted errors in maxlist: 397, perc: 10.461133069828723\n",
      "Q Values for max: [0.174294650517355, 0.19800262615496933, 0.8747671835188299]\n",
      "score: [14, 14, 18, 18, 20, 13, 17, 15, 18, 12, 11, 12, 12, 15, 13, 22, 11, 18, 16, 8, 18, 13, 14, 11, 14, 11, 15, 23, 10, 11]\n",
      "average score: 14.566666666666666\n",
      "Testing penaltylist\n",
      "Number of predicted terminations in penaltylist: 38, perc: 31.40495867768595\n",
      "Q Values for penalty: [0.06685816000931519, 0.09229110772452079, -0.02637632181944926]\n",
      "saving checkpoint: D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\\model\\model-regr_3_0.000001500_10.46.h5\n",
      "4: learning rate: 1.5e-06\n",
      "7.802746566791511\n",
      "loading checkpoint: D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\\model\\model-regr.h5\n",
      "Number of maxrewards in shufflelist: 4405, perc: 20.969200742609605\n",
      "Number of terminations in shufflelist: 151, perc: 0.7188080163754939\n",
      "Testing maxlist\n",
      "Number of predicted errors in maxlist: 1065, perc: 24.177071509648126\n",
      "Q Values for max: [0.20703580286628678, 0.18412410652985908, 0.49810355571896753]\n",
      "score: [15, 6, 11, 17, 10, 12, 14, 11, 13, 18, 16, 17, 15, 11, 11, 20, 14, 18, 19, 17, 18, 15, 17, 7, 14, 13, 15, 15, 14, 15]\n",
      "average score: 14.266666666666667\n",
      "Testing penaltylist\n",
      "Number of predicted terminations in penaltylist: 54, perc: 35.76158940397351\n",
      "Q Values for penalty: [0.09869751154093553, 0.09148209991044556, -0.1998819113024418]\n",
      "saving checkpoint: D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\\model\\model-regr_4_0.000001500_24.18.h5\n",
      "5: learning rate: 7.5e-07\n",
      "7.802746566791511\n",
      "loading checkpoint: D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\\model\\model-regr.h5\n",
      "Number of maxrewards in shufflelist: 5121, perc: 21.151542687208295\n",
      "Number of terminations in shufflelist: 181, perc: 0.7475940688116972\n",
      "Testing maxlist\n",
      "Number of predicted errors in maxlist: 544, perc: 10.622925209919938\n",
      "Q Values for max: [0.19904618668378132, 0.2034885452096757, 0.7935000024917791]\n",
      "score: [9, 16, 15, 13, 14, 15, 17, 16, 14, 14, 10, 12, 12, 9, 12, 18, 9, 19, 23, 16, 9, 11, 10, 18, 18, 12, 14, 12, 18, 10]\n",
      "average score: 13.833333333333334\n",
      "Testing penaltylist\n",
      "Number of predicted terminations in penaltylist: 61, perc: 33.70165745856354\n",
      "Q Values for penalty: [0.012917794822329316, 0.053872319504371664, -0.09026611334958129]\n",
      "saving checkpoint: D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\\model\\model-regr_5_0.000000750_10.62.h5\n",
      "6: learning rate: 7.5e-07\n",
      "7.802746566791511\n",
      "loading checkpoint: D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\\model\\model-regr.h5\n",
      "Number of maxrewards in shufflelist: 5804, perc: 21.18634787369958\n",
      "Number of terminations in shufflelist: 211, perc: 0.7702135426172659\n",
      "Testing maxlist\n",
      "Number of predicted errors in maxlist: 804, perc: 13.852515506547208\n",
      "Q Values for max: [0.17270887574100088, 0.20529604424497203, 0.7689014286596166]\n",
      "score: [14, 18, 15, 12, 13, 13, 16, 15, 12, 8, 14, 13, 11, 13, 21, 20, 10, 9, 18, 15, 17, 17, 14, 16, 20, 15, 13, 14, 18, 19]\n",
      "average score: 14.766666666666667\n",
      "Testing penaltylist\n",
      "Number of predicted terminations in penaltylist: 61, perc: 28.90995260663507\n",
      "Q Values for penalty: [0.09118278287527685, 0.07431082095580079, -0.020772298524306284]\n",
      "saving checkpoint: D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\\model\\model-regr_6_0.000000750_13.85.h5\n",
      "7: learning rate: 7.5e-07\n",
      "7.802746566791511\n",
      "loading checkpoint: D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\\model\\model-regr.h5\n",
      "Number of maxrewards in shufflelist: 6381, perc: 20.917882314374694\n",
      "Number of terminations in shufflelist: 241, perc: 0.7900344205867891\n",
      "Testing maxlist\n",
      "Number of predicted errors in maxlist: 973, perc: 15.248393668703965\n",
      "Q Values for max: [0.23467268689163817, 0.20801408606517316, 0.7492901208715493]\n",
      "score: [12, 14, 18, 18, 14, 9, 15, 16, 9, 15, 7, 12, 8, 19, 19, 12, 15, 11, 21, 11, 15, 18, 10, 16, 15, 16, 14, 16, 15, 19]\n",
      "average score: 14.3\n",
      "Testing penaltylist\n",
      "Number of predicted terminations in penaltylist: 103, perc: 42.738589211618255\n",
      "Q Values for penalty: [0.0004800390993163793, 0.07815641306989915, -0.36080062049305783]\n",
      "saving checkpoint: D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\\model\\model-regr_7_0.000000750_15.25.h5\n",
      "8: learning rate: 3.75e-07\n",
      "7.802746566791511\n",
      "loading checkpoint: D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\\model\\model-regr.h5\n",
      "Number of maxrewards in shufflelist: 7127, perc: 21.120165950511186\n",
      "Number of terminations in shufflelist: 271, perc: 0.8030819380648985\n",
      "Testing maxlist\n",
      "Number of predicted errors in maxlist: 1721, perc: 24.147607689069734\n",
      "Q Values for max: [0.21617563529366343, 0.19083170005268915, 0.5863487776261703]\n",
      "score: [14, 10, 11, 19, 12, 18, 9, 13, 17, 15, 7, 21, 17, 13, 15, 7, 16, 14, 15, 14, 12, 13, 16, 21, 9, 9, 13, 23, 12, 13]\n",
      "average score: 13.933333333333334\n",
      "Testing penaltylist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predicted terminations in penaltylist: 110, perc: 40.59040590405904\n",
      "Q Values for penalty: [0.10880433784529732, 0.13178867570423553, -0.14156545655341385]\n",
      "saving checkpoint: D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\\model\\model-regr_8_0.000000375_24.15.h5\n",
      "9: learning rate: 3.75e-07\n",
      "7.802746566791511\n",
      "loading checkpoint: D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\\model\\model-regr.h5\n"
     ]
    }
   ],
   "source": [
    "run_game(1.5e-06, 20, 60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
