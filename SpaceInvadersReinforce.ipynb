{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os.path\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Flatten, ZeroPadding2D, UpSampling2D\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import pandas as pd \n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from scipy.signal import savgol_filter\n",
    "import math\n",
    "\n",
    "pathname = r\"D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\"\n",
    "datadirname = \"data\"\n",
    "testdirname = \"test\"\n",
    "validdirname = \"valid\"\n",
    "modeldirname = \"model\"\n",
    "datacsvname = \"data.csv\"\n",
    "modeljsonname=\"model-regr.json\"\n",
    "modelweightname=\"model-regr.h5\"\n",
    "dim = (50,50) \n",
    "actionstonum = {\"RIGHT\": 0,\n",
    "           \"LEFT\": 1,\n",
    "           \"SPACE\" : 2,\n",
    "          }\n",
    "numtoactions = {0: \"RIGHT\",\n",
    "           1: \"LEFT\",\n",
    "           2: \"SPACE\",\n",
    "          }\n",
    "scores = []\n",
    "overallscores = []\n",
    "manual = False\n",
    "\n",
    "\n",
    "def create_q_model():\n",
    "        # Network defined by the Deepmind paper\n",
    "        inputs = layers.Input(shape=(dim[0], dim[1], 3,))\n",
    "\n",
    "        # Convolutions on the frames on the screen\n",
    "        layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "        layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "        layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "\n",
    "        layer4 = layers.Flatten()(layer3)\n",
    "\n",
    "        layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "        action = layers.Dense(4, activation=\"linear\")(layer5)\n",
    "\n",
    "        return keras.Model(inputs=inputs, outputs=action)\n",
    "\n",
    "def run_game(learning_rate = 1.5e-06, epochs = 5, benchmin = 68.0):\n",
    "    manual = False\n",
    "    lr = [learning_rate for i in range(epochs)]\n",
    "\n",
    "    iterations = len(lr)\n",
    "    benches = []\n",
    "    qms = []\n",
    "    qps = []\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(iterations):\n",
    "        print(f\"{i}: learning rate: {lr[i]}\")\n",
    "        print(benchmin)\n",
    "        k = 2\n",
    "        game = Game(500,500)\n",
    "        game.load_replay_memory()\n",
    "        for j in range(k):\n",
    "            game.initialize()\n",
    "            game.run(j)\n",
    "        bench, qm, qp = game.print_benchmark()\n",
    "        benches.append(bench)\n",
    "        qms.append(qm)\n",
    "        qps.append(qp)\n",
    "        game.save_replay_memory()\n",
    "        game.save_checkpoint(f\"model-regr_{i}_{lr[i]:.9f}_{bench:.2f}.h5\")\n",
    "        if bench < benchmin:\n",
    "            benchmin = bench\n",
    "            game.save_checkpoint()\n",
    "        else:\n",
    "            counter += 1\n",
    "        if counter == 3:\n",
    "            counter = 0\n",
    "            lr = [i*0.5 for i in lr] \n",
    "            \n",
    "        overallscore = game.print_overall_score()\n",
    "        overallscores.append(overallscore)\n",
    "    return benches, qms, qps\n",
    "\n",
    "model = create_q_model()\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(pathname, modeldirname,modeljsonname), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(os.path.join(pathname, modeldirname,modelweightname))\n",
    "\n",
    "\n",
    "class Game:\n",
    "    screen = None\n",
    "    \n",
    "    lost = False\n",
    "    done = False\n",
    "\n",
    "    def __init__(self, width, height, lr=1e-3, checkpointparname=\"model-regr.h5\"):\n",
    "        \n",
    "        self.currentAction = \"\"\n",
    "                       \n",
    "        self.shufflelist = []\n",
    "        \n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.screen = pygame.display.set_mode((int(width), int(height)))\n",
    "        self.screen.fill([255,0,0])\n",
    "\n",
    "        self.imgresh1 = None\n",
    "        self.imgresh2 = None\n",
    "\n",
    "        self.reward = 0\n",
    "        self.MAXREWARD = 1.0\n",
    "        self.PENALTY = -1.0\n",
    "        self.FIREPENALTY = 0.0\n",
    "        self.MOVEPENALTY = 0.0\n",
    "        \n",
    "        self.BATCHSIZE = 19\n",
    "        self.DISCOUNT = 0.99\n",
    "        self.ALPHA = 0.3\n",
    "        \n",
    "        manual=False\n",
    "        if manual == True:\n",
    "            self.EPSILON = 0.999\n",
    "        else:\n",
    "            self.EPSILON = 0.3\n",
    "        \n",
    "        self.REPLAYSIZE = 40_000\n",
    "        self.overall_score = 0\n",
    "        self.overall_numbatches = 0\n",
    "        self.overall_accumulatedstates = np.array([0.0,0.0,0.0,0.0])\n",
    "        \n",
    "        \n",
    "        self.path = os.path.join(pathname, datadirname)\n",
    "        self.modelpath =  os.path.join(pathname, modeldirname)\n",
    "        \n",
    "        self.filename = \"data.csv\"\n",
    "        \n",
    "        self.model = create_q_model()\n",
    "        self.model_target = create_q_model()\n",
    "\n",
    "        self.learningrate = lr\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=self.learningrate, clipnorm=1.0)\n",
    "        self.loss_function = keras.losses.Huber()\n",
    "\n",
    "        self.checkpointname = os.path.join(pathname, modeldirname,checkpointparname)\n",
    "        print(f\"loading checkpoint: {self.checkpointname}\")\n",
    "        self.model_target.load_weights(self.checkpointname)\n",
    "        \n",
    "        self.overall_scores=[]\n",
    "        self.checkpoint_counter=0\n",
    "        \n",
    "        self.debugcounter = 0\n",
    "\n",
    "        \n",
    "    def initialize(self):\n",
    "        pygame.init()\n",
    "        self.aliens = []\n",
    "        self.rockets = []\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.hero = Hero(self, self.width / 2, self.height - 20)\n",
    "        \n",
    "        #Generator\n",
    "        margin = 30  # mezera od okraju obrazovky\n",
    "        width = 50  # mezera mezi alieny\n",
    "        self.screen = pygame.display.set_mode((int(self.width), int(self.height)))\n",
    "        for x in range(margin, self.width - margin, width):\n",
    "            for y in range(margin, int(self.height / 2), width):\n",
    "                if(random.randint(0,1)==1):\n",
    "                    self.aliens.append(Alien(self, x, y))\n",
    "                    \n",
    "        self.rocket = None\n",
    "\n",
    "    def run(self, i_index):\n",
    "        i = i_index + self.get_maxi() + 1\n",
    "        j = 0\n",
    "        while True:\n",
    "            img1 = np.frombuffer(pygame.image.tostring(self.screen, \"RGB\"), dtype=np.uint8)\n",
    "            self.imgresh1 = np.reshape(img1,(self.width,self.height, 3))\n",
    "            self.imgresh1 = cv2.resize(self.imgresh1, dim, interpolation = cv2.INTER_NEAREST )\n",
    "            self.imgresh1 = cv2.cvtColor(self.imgresh1, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            current_state = np.array(self.imgresh1, dtype=np.float32)/255.0\n",
    "            state_tensor = tf.convert_to_tensor(current_state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = self.model(state_tensor, training=False)\n",
    "            theaction = tf.argmax(action_probs[0]).numpy()\n",
    "            # model action > 2\n",
    "            if (theaction > 2):\n",
    "                theaction = 2\n",
    "\n",
    "            #win\n",
    "            if len(self.aliens) == 0:\n",
    "                pygame.display.flip()                         \n",
    "                pygame.quit()\n",
    "                return\n",
    "\n",
    "            pressed = pygame.key.get_pressed()\n",
    "            if pressed[pygame.K_LEFT]:  # sipka doleva\n",
    "                self.currentAction = \"LEFT\"\n",
    "                \n",
    "            elif pressed[pygame.K_RIGHT]:  # sipka doprava\n",
    "                self.currentAction = \"RIGHT\"\n",
    "                \n",
    "            elif pressed[pygame.K_q]:\n",
    "                pygame.display.flip()                         \n",
    "                pygame.quit()\n",
    "                return\n",
    "\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.KEYDOWN and event.key == pygame.K_SPACE and not self.lost:\n",
    "                    self.currentAction = \"SPACE\"\n",
    "                    \n",
    "            if np.random.random() > self.EPSILON:\n",
    "                self.currentAction = numtoactions[theaction]\n",
    "            #else:\n",
    "                #if manual != True:\n",
    "                    #self.currentAction = self.get_direction();\n",
    "                #assert actionstonum[self.currentAction] >= 0\n",
    "                #assert actionstonum[self.currentAction] < 3\n",
    "                \n",
    "            if self.currentAction == \"RIGHT\":\n",
    "                self.hero.x += 2 if self.hero.x < self.width - 20 else 0  # prava hranice\n",
    "                self.reward = self.MOVEPENALTY\n",
    "                \n",
    "            if self.currentAction == \"LEFT\":\n",
    "                self.hero.x -= 2 if self.hero.x > 20 else 0  # leva hranice plochy\n",
    "                self.reward = self.MOVEPENALTY\n",
    "                \n",
    "            if self.currentAction == \"SPACE\":\n",
    "                self.reward = self.FIREPENALTY\n",
    "                self.rockets.append(Rocket(self, self.hero.x, self.hero.y))\n",
    "\n",
    "            pygame.display.flip()\n",
    "            self.clock.tick(60)\n",
    "            self.screen.fill((255, 0, 0))\n",
    "\n",
    "            for alien in self.aliens:\n",
    "                alien.draw()\n",
    "                if(alien.checkCollision(self)):\n",
    "                    self.reward = self.MAXREWARD\n",
    "                if (alien.y > (self.height-40)):\n",
    "                    self.reward = self.PENALTY\n",
    "                    pygame.display.flip()                         \n",
    "                    pygame.quit()\n",
    "                    return\n",
    "                    \n",
    "            for rocket in self.rockets:\n",
    "                rocket.draw()\n",
    "\n",
    "            if not self.lost: self.hero.draw()\n",
    "                \n",
    "            img2 = np.frombuffer(pygame.image.tostring(self.screen, \"RGB\"), dtype=np.uint8)\n",
    "            self.imgresh2 = np.reshape(img2,(self.width,self.height, 3))\n",
    "            self.imgresh2 = cv2.resize(img2, dim, interpolation = cv2.INTER_NEAREST )\n",
    "            self.imgresh2 = cv2.cvtColor(self.imgresh2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            self.write(i,j)\n",
    "\n",
    "            j+=1\n",
    "\n",
    "    def write(self, i, j): \n",
    "\n",
    "        cv2.imwrite(os.path.join(self.path,\"current_{}_{}.png\".format(i,j)), self.imgresh1)\n",
    "        cv2.imwrite(os.path.join(self.path,\"next_{}_{}.png\".format(i,j)), self.imgresh2)\n",
    "\n",
    "    def train(self, i, j, term):\n",
    "        \n",
    "        # https://pythonprogramming.net/training-deep-q-learning-dqn-reinforcement-learning-python-tutorial/\n",
    "        \n",
    "        currentstate = \"current_{}_{}.png\".format(i,j)\n",
    "\n",
    "        nextstate = \"next_{}_{}.png\".format(i,j)      \n",
    "        \n",
    "        batch, files = self.pop_batch(self.BATCHSIZE)\n",
    "        \n",
    "        assert(self.imgresh1.shape == (dim[0], dim[1],3))\n",
    "        assert(self.imgresh2.shape == (dim[0], dim[1],3))\n",
    "        \n",
    "        batch.append([self.imgresh1, actionstonum[self.changeto], self.reward, self.imgresh2, term, self.snake_pos[0], self.snake_pos[1], self.food_pos[0], self.food_pos[1]])\n",
    "        files.append((\"current_{}_{}.png\".format(i,j), \"next_{}_{}.png\".format(i,j)))\n",
    "        \n",
    "        self.write(i,j)\n",
    "         \n",
    "        self.backprop(batch)\n",
    "        \n",
    "        self.numbatches += 1\n",
    "            \n",
    "        self.push_batch(batch, files)   \n",
    "  \n",
    "        return    \n",
    "    \n",
    "    def get_maxi(self):\n",
    "        \n",
    "        maxi = 0\n",
    "        \n",
    "        for item in self.shufflelist:\n",
    "            curr = item[0]\n",
    "            s = re.findall(r'\\d+', curr)[0]\n",
    "            if int(s) > maxi:\n",
    "                maxi = int(s)\n",
    "        \n",
    "        return maxi\n",
    "    \n",
    "    def load_replay_memory(self):\n",
    "\n",
    "        f = open(os.path.join(os.path.join(self.path,datacsvname)), \"r\")\n",
    "        \n",
    "        df = pd.read_csv(f, index_col = 0) \n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "\n",
    "            currentpicname = row[\"currentstate\"]\n",
    "            action = actionstonum[row[\"action\"]]\n",
    "            reward = row[\"reward\"]\n",
    "            nextpicname = row[\"nextstate\"]\n",
    "            terminated = row[\"terminated\"]\n",
    "\n",
    "            assert os.path.isfile(os.path.join(self.path,currentpicname)) == True\n",
    "            assert (action < 5 and action >= 0)\n",
    "            assert isinstance(reward,int) or isinstance(reward, float)\n",
    "            assert os.path.isfile(os.path.join(self.path,nextpicname)) == True\n",
    "            \n",
    "            self.shufflelist.append([currentpicname,action,reward,nextpicname, terminated])\n",
    "\n",
    "        random.shuffle(self.shufflelist)\n",
    "        \n",
    "        #print(self.shufflelist)\n",
    "\n",
    "        #print(f\"loading: size of replay memory {len(self.shufflelist)}\")\n",
    "        \n",
    "        f.close()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def save_replay_memory(self):\n",
    "        \n",
    "        assert os.path.isfile(os.path.join(self.path,datacsvname)) == True\n",
    "        \n",
    "        data = []\n",
    "        \n",
    "        if len(self.shufflelist) == 0:\n",
    "            return\n",
    "        \n",
    "        if len(self.shufflelist) > self.REPLAYSIZE:\n",
    "            \n",
    "            self.numbatches = len(self.shufflelist) - self.REPLAYSIZE\n",
    "            self.overall_numbatches += self.numbatches\n",
    "            \n",
    "            for i in range(len(self.shufflelist) - self.REPLAYSIZE):\n",
    "                item = self.shufflelist.pop(0)\n",
    "                assert os.path.isfile(os.path.join(self.path,item[0])) == True\n",
    "                assert os.path.isfile(os.path.join(self.path,item[3])) == True\n",
    "                os.remove(os.path.join(self.path,item[0]))\n",
    "                os.remove(os.path.join(self.path,item[3]))\n",
    "                \n",
    "        for (cs, act, rew, fs, term) in self.shufflelist:\n",
    "            \n",
    "            data.append({'currentstate': cs, 'action': numtoactions[act], 'reward': rew, 'nextstate': fs, 'terminated': term})\n",
    "            \n",
    "        df = pd.DataFrame(data) \n",
    "        \n",
    "        df.to_csv(os.path.join(self.path, self.filename)) \n",
    "        \n",
    "        #print(f\"saving: size of replay memory {len(self.shufflelist)}\")\n",
    "    \n",
    "        return\n",
    "    \n",
    "    def print_benchmark(self):\n",
    "\n",
    "        maxlist = []\n",
    "        penaltylist = []\n",
    "        averagestates = [0,0,0,0]\n",
    "        averagepenalty = [0,0,0,0]\n",
    "        pmerror = 0\n",
    "        pterror = 0\n",
    "\n",
    "        for (cs, act, rew, fs, term) in self.shufflelist:\n",
    "            if rew == self.MAXREWARD or rew == 30.0:\n",
    "                maxlist.append((cs,act,rew,fs,term))\n",
    "            if rew == self.PENALTY:\n",
    "                penaltylist.append((cs,act,rew,fs,term))\n",
    "        print(f\"Number of maxrewards in shufflelist: {len(maxlist)}, perc: {100*len(maxlist)/len(self.shufflelist)}\")\n",
    "        print(f\"Number of terminations in shufflelist: {len(penaltylist)}, perc: {100*len(penaltylist)/len(self.shufflelist)}\")\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        print(\"Testing maxlist\")\n",
    "        for i in range(len(maxlist)):\n",
    "            img = cv2.imread(os.path.join(pathname, datadirname, maxlist[i][0]),cv2.IMREAD_COLOR )\n",
    "            states = self.model.predict(np.array([img])/255.0, batch_size=1, verbose=0)[0]\n",
    "            averagestates += states\n",
    "            if np.argmax(states) != maxlist[i][1]:\n",
    "                count += 1\n",
    "            pmerror = 100*count/len(maxlist)\n",
    "        print(f\"Number of predicted errors in maxlist: {count}, perc: {pmerror}\")\n",
    "        print(f\"Q Values for max: {averagestates/len(maxlist)}\")\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        print(\"Testing penaltylist\") \n",
    "        for i in range(len(penaltylist)):\n",
    "            img = cv2.imread(os.path.join(pathname, datadirname, penaltylist[i][0]),cv2.IMREAD_COLOR )\n",
    "            states = self.model.predict(np.array([img])/255.0, batch_size=1, verbose=0)[0]\n",
    "            averagepenalty += states\n",
    "            if np.argmax(states) == penaltylist[i][1]:\n",
    "                count += 1\n",
    "            pterror = 100*count/len(penaltylist)\n",
    "        print(f\"Number of predicted terminations in penaltylist: {count}, perc: {pterror}\")\n",
    "        print(f\"Q Values for penalty: {[i/len(penaltylist) for i in averagepenalty]}\")\n",
    "        \n",
    "        return pmerror, [i/len(maxlist) for i in averagestates], [i/len(penaltylist) for i in averagepenalty]\n",
    "    \n",
    "    def save_checkpoint(self, checkpointparname=modelweightname):\n",
    "                                                                         \n",
    "        self.model_target.set_weights(self.model.get_weights())\n",
    "        print(f\"saving checkpoint: {os.path.join(pathname, modeldirname,checkpointparname)}\")\n",
    "        self.model_target.save_weights(os.path.join(pathname, modeldirname,checkpointparname) )\n",
    "            \n",
    "        return\n",
    "\n",
    "    def print_score(self):\n",
    "        print(f\" ----> TIME IS {datetime.now():%Y-%m-%d_%H-%M-%S}\")\n",
    "        print(f\" ----> SCORE is {self.score}\")\n",
    "        print(f\" ----> NUM OF BATCHES is {self.numbatches}\")\n",
    "        return self.score, self.numbatches\n",
    "    \n",
    "    def print_overall_score(self):\n",
    "        print(f\"--> TIME IS {datetime.now():%Y-%m-%d_%H-%M-%S}\")\n",
    "        print(f\"--> OVERALL SCORE is {self.overall_score}\")\n",
    "        print(f\"--> OVERALL NUM OF BATCHES is {self.overall_numbatches}\")\n",
    "        return self.overall_score, self.overall_numbatches     \n",
    "    \n",
    "\n",
    "\n",
    "class Alien:\n",
    "    def __init__(self, game, x, y):\n",
    "        self.x = x\n",
    "        self.game = game\n",
    "        self.y = y\n",
    "        self.size = 40\n",
    "\n",
    "    def draw(self):\n",
    "        pygame.draw.rect(self.game.screen,(0, 0, 255),  # barva objektu\n",
    "                         pygame.Rect(self.x, self.y, self.size, self.size))\n",
    "        self.y += 0.4\n",
    "\n",
    "    def checkCollision(self, game):\n",
    "        for rocket in game.rockets:\n",
    "            if (rocket.x < self.x + self.size and\n",
    "                    rocket.x > self.x - self.size and\n",
    "                    rocket.y < self.y + self.size and\n",
    "                    rocket.y > self.y - self.size):\n",
    "                game.rockets.remove(rocket)\n",
    "                game.aliens.remove(self)\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "\n",
    "class Hero:\n",
    "    def __init__(self, game, x, y):\n",
    "        self.x = x\n",
    "        self.game = game\n",
    "        self.y = y\n",
    "\n",
    "    def draw(self):\n",
    "        pygame.draw.rect(self.game.screen,\n",
    "                         (255, 255, 255),\n",
    "                         pygame.Rect(self.x, self.y, 40, 20))\n",
    "\n",
    "\n",
    "class Rocket:\n",
    "    def __init__(self, game, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.game = game\n",
    "\n",
    "    def draw(self):\n",
    "        pygame.draw.rect(self.game.screen,  # renderovací plocha\n",
    "                         (0, 255, 0),  # barva objektu\n",
    "                         pygame.Rect(self.x, self.y, 15, 15))\n",
    "        self.y -= 2  # poletí po herní ploše nahoru 2px/snímek\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    " #   game = Game(500, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: learning rate: 1.5e-06\n",
      "60.0\n",
      "loading checkpoint: D:\\OneDrive - Hochschule Albstadt-Sigmaringen\\Studium\\Semester 5\\DesignCPS\\model\\model-regr.h5\n",
      "Number of maxrewards in shufflelist: 3, perc: 14.285714285714286\n",
      "Number of terminations in shufflelist: 0, perc: 0.0\n",
      "Testing maxlist\n",
      "Number of predicted errors in maxlist: 2, perc: 66.66666666666667\n",
      "Q Values for max: [ 0.0017767   0.00116628  0.00433651 -0.00233647]\n",
      "Testing penaltylist\n",
      "Number of predicted terminations in penaltylist: 0, perc: 0\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-a0848bd925eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrun_game\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.5e-06\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m60.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-ded5ae1a6190>\u001b[0m in \u001b[0;36mrun_game\u001b[1;34m(learning_rate, epochs, benchmin)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m         \u001b[0mbench\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_benchmark\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[0mbenches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbench\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m         \u001b[0mqms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-ded5ae1a6190>\u001b[0m in \u001b[0;36mprint_benchmark\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    426\u001b[0m             \u001b[0mpterror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpenaltylist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Number of predicted terminations in penaltylist: {count}, perc: {pterror}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Q Values for penalty: {[i/len(penaltylist) for i in averagepenalty]}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpmerror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maveragestates\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpenaltylist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maveragepenalty\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-ded5ae1a6190>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    426\u001b[0m             \u001b[0mpterror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpenaltylist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Number of predicted terminations in penaltylist: {count}, perc: {pterror}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Q Values for penalty: {[i/len(penaltylist) for i in averagepenalty]}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpmerror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxlist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maveragestates\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpenaltylist\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0maveragepenalty\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "run_game(1.5e-06, 5, 60.0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
