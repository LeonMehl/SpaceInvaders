{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://keras.io/examples/rl/deep_q_network_breakout/\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, BatchNormalization, Activation, Dense, Dropout, Flatten, ZeroPadding2D, UpSampling2D\n",
    "from keras.layers.core import Lambda, RepeatVector, Reshape\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D, GlobalMaxPool2D\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame, sys, time, random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import base64\n",
    "import os.path\n",
    "import pandas as pd \n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from scipy.signal import savgol_filter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = (50,50) \n",
    "pathname = r\"C:/Users/Caspar/Desktop/Reinforcement Learning\"\n",
    "datadirname = \"data\"\n",
    "validdirname = \"valid\"\n",
    "modeldirname = \"model\"\n",
    "datacsvname = \"data.csv\"\n",
    "modeljsonname=\"model-regr.json\"\n",
    "modelweightname=\"model-regr.h5\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "actionstonum = {\"RIGHT\": 0,\n",
    "           \"LEFT\": 1,\n",
    "           \"UP\" : 2,\n",
    "           \"DOWN\" : 3,\n",
    "          }\n",
    "numtoactions = {0: \"RIGHT\",\n",
    "           1: \"LEFT\",\n",
    "           2: \"UP\",\n",
    "           3: \"DOWN\",\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_q_model():\n",
    "    # Network defined by the Deepmind paper\n",
    "    inputs = layers.Input(shape=(dim[0], dim[1], 3,))\n",
    "\n",
    "    # Convolutions on the frames on the screen\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    "\n",
    "    layer4 = layers.Flatten()(layer3)\n",
    "\n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "    action = layers.Dense(4, activation=\"linear\")(layer5)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_q_model()\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(pathname, modeldirname,modeljsonname), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(os.path.join(pathname, modeldirname,modelweightname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "\n",
    "\n",
    "    def __init__(self, lr=1e-3, checkpointparname=modelweightname):\n",
    "        \n",
    "        self.speed = 80\n",
    "\n",
    "        self.frame_size_x = 200\n",
    "        self.frame_size_y = 200\n",
    "\n",
    "        self.black = pygame.Color(0, 0, 0)\n",
    "        self.white = pygame.Color(255, 255, 255)\n",
    "        self.green = pygame.Color(0, 255, 0)\n",
    "        self.mag = pygame.Color(255, 0, 255)\n",
    "        \n",
    "        self.imgresh1 = None\n",
    "        self.imgresh2 = None\n",
    "        \n",
    "        self.reward = 0\n",
    "        self.MAXREWARD = 1.0\n",
    "        self.PENALTY = -1.0\n",
    "        self.MOVEPENALTY = 0.0\n",
    "        \n",
    "        self.BATCHSIZE = 19\n",
    "        self.DISCOUNT = 0.99\n",
    "        self.ALPHA = 0.3\n",
    "        \n",
    "        if manual == True:\n",
    "            self.EPSILON = 0.999\n",
    "        else:\n",
    "            self.EPSILON = 0.3\n",
    "        \n",
    "        self.REPLAYSIZE = 40_000\n",
    "        self.overall_score = 0\n",
    "        self.overall_numbatches = 0\n",
    "        self.overall_accumulatedstates = np.array([0.0,0.0,0.0,0.0])\n",
    "        \n",
    "        \n",
    "        self.path = os.path.join(pathname, datadirname)\n",
    "        self.modelpath =  os.path.join(pathname, modeldirname)\n",
    "        \n",
    "        self.filename = \"data.csv\"\n",
    "        \n",
    "        self.model = create_q_model()\n",
    "        self.model_target = create_q_model()\n",
    "\n",
    "        self.learningrate = lr\n",
    "        self.optimizer = keras.optimizers.Adam(learning_rate=self.learningrate, clipnorm=1.0)\n",
    "        self.loss_function = keras.losses.Huber()\n",
    "\n",
    "        self.checkpointname = os.path.join(pathname, modeldirname,checkpointparname)\n",
    "        print(f\"loading checkpoint: {self.checkpointname}\")\n",
    "        self.model_target.load_weights(self.checkpointname)\n",
    "        \n",
    "        self.overall_scores=[]\n",
    "        self.checkpoint_counter=0\n",
    "        \n",
    "        self.shufflelist = []\n",
    "        self.debugcounter = 0\n",
    "        \n",
    "    def initialize(self, i, j):\n",
    "\n",
    "        status = pygame.init()\n",
    "\n",
    "        if status[1] > 0:\n",
    "            print(f'Number of Errors: {status[1]} ...')\n",
    "            sys.exit(-1)\n",
    "\n",
    "\n",
    "        # Initialise game window\n",
    "        pygame.display.set_caption(f\"{i}-{j}\")\n",
    "        self.game_window = pygame.display.set_mode((self.frame_size_x, self.frame_size_y)) \n",
    "\n",
    "        \n",
    "        self.controller = pygame.time.Clock()\n",
    "   \n",
    "        posx = (random.randint(40,160)//10)*10\n",
    "        posy = (random.randint(40,160)//10)*10\n",
    "           \n",
    "        self.snake_pos = [posx, posy]\n",
    "        self.snake_body = [[posx, posy], [posx-10, posy], [posx-(2*10), posy]]\n",
    "\n",
    "        self.food_pos = [random.randrange(1, (self.frame_size_x//10)) * 10, random.randrange(1, (self.frame_size_y//10)) * 10]\n",
    "        self.food_spawn = True\n",
    "\n",
    "        self.direction = 'RIGHT'\n",
    "        self.changeto = self.direction\n",
    "\n",
    "        self.score = 0\n",
    "        self.numbatches = 0\n",
    "\n",
    "        self.event_happened = False\n",
    "        \n",
    "        self.model.load_weights(self.checkpointname)\n",
    "\n",
    "            \n",
    "    def run(self, i_index):\n",
    "        \n",
    "        i = i_index + self.get_maxi() + 1\n",
    "        j = 0\n",
    "\n",
    "        while True:\n",
    "            \n",
    "            img1 = np.frombuffer(pygame.image.tostring(self.game_window, \"RGB\"), dtype=np.uint8)\n",
    "            self.imgresh1 = np.reshape(img1,(self.frame_size_x,self.frame_size_y, 3))\n",
    "            self.imgresh1 = cv2.resize(self.imgresh1, dim, interpolation = cv2.INTER_NEAREST )\n",
    "            #current_state = [self.imgresh1]\n",
    "            #astate = self.model.predict(np.array(current_state)/255.0, batch_size=1, verbose=0)[0]  \n",
    "            #theaction = np.argmax(astate)\n",
    "            \n",
    "            current_state = np.array(self.imgresh1, dtype=np.float32)/255.0\n",
    "            state_tensor = tf.convert_to_tensor(current_state)\n",
    "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "            action_probs = self.model(state_tensor, training=False)\n",
    "            theaction = tf.argmax(action_probs[0]).numpy()\n",
    "            \n",
    "            assert theaction < 5 and theaction >= 0\n",
    "            \n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    pygame.quit()\n",
    "                    return\n",
    "                # Whenever a key is pressed down\n",
    "                elif event.type == pygame.KEYDOWN:\n",
    "\n",
    "                    if event.key == pygame.K_UP or event.key == ord('w'):\n",
    "                        self.changeto = 'UP'\n",
    "                    if event.key == pygame.K_DOWN or event.key == ord('s'):\n",
    "                        self.changeto = 'DOWN'\n",
    "                    if event.key == pygame.K_LEFT or event.key == ord('a'):\n",
    "                        self.changeto = 'LEFT'\n",
    "                    if event.key == pygame.K_RIGHT or event.key == ord('d'):\n",
    "                        self.changeto = 'RIGHT'\n",
    "                    \n",
    "                    # Esc -> Create event to quit the game\n",
    "                    if event.key == pygame.K_ESCAPE:\n",
    "                        pygame.event.post(pygame.event.Event(pygame.QUIT))\n",
    "\n",
    "\n",
    "            if np.random.random() > self.EPSILON:\n",
    "                self.changeto = numtoactions[theaction]\n",
    "            else:\n",
    "                if manual != True:\n",
    "                    #self.changeto = numtoactions[np.random.randint(0, len(actionstonum))]\n",
    "                    self.changeto = self.get_direction();\n",
    "                assert actionstonum[self.changeto] >= 0\n",
    "                assert actionstonum[self.changeto] < 5\n",
    "                    \n",
    "            if self.changeto == 'UP' and self.direction != 'DOWN':\n",
    "                self.direction = 'UP'\n",
    "            if self.changeto == 'DOWN' and self.direction != 'UP':\n",
    "                self.direction = 'DOWN'\n",
    "            if self.changeto == 'LEFT' and self.direction != 'RIGHT':\n",
    "                self.direction = 'LEFT'\n",
    "            if self.changeto == 'RIGHT' and self.direction != 'LEFT':\n",
    "                self.direction = 'RIGHT'\n",
    "\n",
    "            if self.direction == 'UP':\n",
    "                self.snake_pos[1] -= 10\n",
    "            if self.direction == 'DOWN':\n",
    "                self.snake_pos[1] += 10\n",
    "            if self.direction == 'LEFT':\n",
    "                self.snake_pos[0] -= 10\n",
    "            if self.direction == 'RIGHT':\n",
    "                self.snake_pos[0] += 10\n",
    "\n",
    "            self.snake_body.insert(0, list(self.snake_pos))\n",
    "            if self.snake_pos[0] == self.food_pos[0] and self.snake_pos[1] == self.food_pos[1]:\n",
    "                #self.snake_body.pop() # to be deleted\n",
    "                self.score += 1\n",
    "                self.reward = self.MAXREWARD\n",
    "                self.food_spawn = False\n",
    "            else:\n",
    "                self.snake_body.pop()\n",
    "                self.reward = self.MOVEPENALTY\n",
    "                #self.reward = self.get_continuous_move_penalty()\n",
    "                \n",
    "\n",
    "            if not self.food_spawn:\n",
    "                self.food_pos = [random.randrange(1, (self.frame_size_x//10)) * 10, random.randrange(1, (self.frame_size_y//10)) * 10]\n",
    "            self.food_spawn = True\n",
    "\n",
    "            self.game_window.fill(self.black)\n",
    "            n = 0\n",
    "            for pos in self.snake_body:\n",
    "\n",
    "                if n == 0:\n",
    "                    pygame.draw.rect(self.game_window, self.mag, pygame.Rect(pos[0], pos[1], 10, 10))\n",
    "                else:\n",
    "                    pygame.draw.rect(self.game_window, self.green, pygame.Rect(pos[0], pos[1], 10, 10))\n",
    "                n=+1\n",
    "                \n",
    "\n",
    "            pygame.draw.rect(self.game_window, self.white, pygame.Rect(self.food_pos[0], self.food_pos[1], 10, 10))\n",
    "\n",
    "            if self.snake_pos[0] < 0 or self.snake_pos[0] > self.frame_size_x-10:\n",
    "                self.game_over(i,j)\n",
    "                return\n",
    "            if self.snake_pos[1] < 0 or self.snake_pos[1] > self.frame_size_y-10:\n",
    "                self.game_over(i,j)\n",
    "                return\n",
    "\n",
    "            for block in self.snake_body[1:]:\n",
    "                if self.snake_pos[0] == block[0] and self.snake_pos[1] == block[1]:\n",
    "                    self.game_over(i,j)\n",
    "                    return\n",
    "\n",
    "            pygame.display.update()\n",
    "\n",
    "            img2 = np.frombuffer(pygame.image.tostring(self.game_window, \"RGB\"), dtype=np.uint8)\n",
    "            self.imgresh2 = np.reshape(img2,(self.frame_size_x,self.frame_size_y, 3))\n",
    "            self.imgresh2 = cv2.resize(self.imgresh2, dim, interpolation = cv2.INTER_NEAREST )\n",
    "            \n",
    "            self.controller.tick(self.speed)\n",
    "\n",
    "            if j > 0:\n",
    "                if self.reward == self.MAXREWARD:\n",
    "                    self.train(i,j, False)\n",
    "                elif j%4 == 0:\n",
    "                    self.train(i,j, False)\n",
    "      \n",
    "            j += 1\n",
    "            \n",
    "\n",
    "\n",
    "            \n",
    "    def game_over(self,i,j):\n",
    "        self.reward = self.PENALTY\n",
    "\n",
    "        img2 = np.frombuffer(pygame.image.tostring(self.game_window, \"RGB\"), dtype=np.uint8)\n",
    "        self.imgresh2 = np.reshape(img2,(self.frame_size_x,self.frame_size_y, 3))\n",
    "        self.imgresh2 = cv2.resize(self.imgresh2, dim, interpolation = cv2.INTER_NEAREST )\n",
    "        \n",
    "        self.train(i,j, True)\n",
    "        \n",
    "        self.overall_score += self.score\n",
    "        \n",
    "        self.game_window.fill(self.black)\n",
    "        pygame.display.flip()                         \n",
    "        pygame.quit()\n",
    "\n",
    "    def get_continuous_move_penalty(self):\n",
    "        \n",
    "        penalty = 0\n",
    "        \n",
    "        assert self.snake_body[0][0] == self.snake_pos[0]\n",
    "        assert self.snake_body[0][1] == self.snake_pos[1]\n",
    "        \n",
    "        x = self.snake_pos[0] - self.food_pos[0]\n",
    "        y = self.snake_pos[1] - self.food_pos[1]\n",
    "        \n",
    "        distance = math.sqrt(x*x + y*y)\n",
    "        \n",
    "        maxdist = self.frame_size_x//4\n",
    "        \n",
    "        if distance > maxdist:\n",
    "            penalty = self.MOVEPENALTY\n",
    "        else:\n",
    "            penalty = int(100*self.MOVEPENALTY*distance/maxdist)/100.0\n",
    "        \n",
    "        assert penalty >= self.MOVEPENALTY\n",
    "        assert penalty <= 0\n",
    "        \n",
    "        return penalty\n",
    "\n",
    "        \n",
    "\n",
    "    def get_direction(self):\n",
    "        assert self.snake_body[0][0] == self.snake_pos[0]\n",
    "        assert self.snake_body[0][1] == self.snake_pos[1]\n",
    "        \n",
    "        x = self.snake_pos[0] - self.food_pos[0]\n",
    "        x1 = self.snake_body[1][0] - self.food_pos[0]\n",
    "        \n",
    "        y = self.snake_pos[1] - self.food_pos[1]\n",
    "        y1 = self.snake_body[1][1] - self.food_pos[1]\n",
    "        \n",
    "\n",
    "        \n",
    "        direction = None\n",
    "        direction_h = None\n",
    "        direction_v = None\n",
    "\n",
    "        if x > 0:\n",
    "            direction_h = 'LEFT'\n",
    "        else:\n",
    "            direction_h = 'RIGHT'\n",
    "\n",
    "        if y > 0:\n",
    "            direction_v = 'UP'\n",
    "        else:\n",
    "            direction_v = 'DOWN'\n",
    "                           \n",
    "\n",
    "        if abs(x) > abs(y):\n",
    "            direction = direction_h\n",
    "            \n",
    "            if y == y1 and (abs(x) > abs(x1)):\n",
    "                #print(f\"  hit v x: {abs(x)} x1: {abs(x1)} y: {y} y1: {y1}\")\n",
    "                direction = direction_v\n",
    "        else:\n",
    "            direction = direction_v\n",
    "            if x == x1 and (abs(y) > abs(y1)):\n",
    "                #print(f\"  hit h x: {abs(y)} x1: {abs(y1)} y: {x} y1: {x1}\")\n",
    "                direction = direction_h\n",
    "        \n",
    "        assert direction != ''       \n",
    "        \n",
    "        return direction\n",
    "        \n",
    "\n",
    "    def show_score(self, choice, color, font, size):\n",
    "        score_font = pygame.font.SysFont(font, size)\n",
    "        score_surface = score_font.render('Score : ' + str(self.score), True, color)\n",
    "        score_rect = score_surface.get_rect()\n",
    "        if choice == 1:\n",
    "            score_rect.midtop = (self.frame_size_x/10, 15)\n",
    "        else:\n",
    "            score_rect.midtop = (self.frame_size_x/2, self.frame_size_y/1.25)\n",
    "        self.game_window.blit(score_surface, score_rect)\n",
    "        \n",
    "    def write(self, i, j): \n",
    "\n",
    "        cv2.imwrite(os.path.join(self.path,\"current_{}_{}.png\".format(i,j)), self.imgresh1)\n",
    "        cv2.imwrite(os.path.join(self.path,\"next_{}_{}.png\".format(i,j)), self.imgresh2)\n",
    "\n",
    "        \n",
    "    def load_replay_memory(self):\n",
    "\n",
    "        #assert os.path.isfile(os.path.join(self.path,datacsvname)) == True\n",
    "\n",
    "        f = open(os.path.join(os.path.join(self.path,datacsvname)), \"r\")\n",
    "        \n",
    "        df = pd.read_csv(f, index_col = 0) \n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "\n",
    "            currentpicname = row[\"currentstate\"]\n",
    "            action = actionstonum[row[\"action\"]]\n",
    "            reward = row[\"reward\"]\n",
    "            nextpicname = row[\"nextstate\"]\n",
    "            terminated = row[\"terminated\"]\n",
    "\n",
    "\n",
    "            assert os.path.isfile(os.path.join(self.path,currentpicname)) == True\n",
    "            assert (action < 5 and action >= 0)\n",
    "            assert isinstance(reward,int) or isinstance(reward, float)\n",
    "            assert os.path.isfile(os.path.join(self.path,nextpicname)) == True\n",
    "\n",
    "            self.shufflelist.append([currentpicname,action,reward,nextpicname, terminated])\n",
    "            #self.shufflelist.append([currentpicname,action,reward,nextpicname, terminated])\n",
    "\n",
    "        random.shuffle(self.shufflelist)\n",
    "\n",
    "        #print(f\"loading: size of replay memory {len(self.shufflelist)}\")\n",
    "        \n",
    "        f.close()\n",
    "        \n",
    "        return\n",
    "\n",
    "    def save_replay_memory(self):\n",
    "        \n",
    "        assert os.path.isfile(os.path.join(self.path,datacsvname)) == True\n",
    "        \n",
    "        data = []\n",
    "        \n",
    "        if len(self.shufflelist) == 0:\n",
    "            return\n",
    "        \n",
    "        if len(self.shufflelist) > self.REPLAYSIZE:\n",
    "            \n",
    "            self.numbatches = len(self.shufflelist) - self.REPLAYSIZE\n",
    "            self.overall_numbatches += self.numbatches\n",
    "            \n",
    "            for i in range(len(self.shufflelist) - self.REPLAYSIZE):\n",
    "                item = self.shufflelist.pop(0)\n",
    "                assert os.path.isfile(os.path.join(self.path,item[0])) == True\n",
    "                assert os.path.isfile(os.path.join(self.path,item[3])) == True\n",
    "                os.remove(os.path.join(self.path,item[0]))\n",
    "                os.remove(os.path.join(self.path,item[3]))\n",
    "                \n",
    "        for (cs, act, rew, fs, term) in self.shufflelist:\n",
    "            \n",
    "            data.append({'currentstate': cs, 'action': numtoactions[act], 'reward': rew, 'nextstate': fs, 'terminated': term})\n",
    "            \n",
    "        df = pd.DataFrame(data) \n",
    "        \n",
    "        df.to_csv(os.path.join(self.path, self.filename)) \n",
    "        \n",
    "        #print(f\"saving: size of replay memory {len(self.shufflelist)}\")\n",
    "    \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def pop_batch(self, batchsize):\n",
    "       \n",
    "        batch = []\n",
    "        files = []\n",
    "    \n",
    "        for i in range(batchsize):\n",
    "            \n",
    "            item = self.shufflelist.pop(0)\n",
    "            \n",
    "            assert os.path.isfile(os.path.join(self.path, item[0]))\n",
    "            assert os.path.isfile(os.path.join(self.path, item[3]))\n",
    "            img1 = cv2.imread(os.path.join(self.path, item[0]),cv2.IMREAD_COLOR )\n",
    "            img2 = cv2.imread(os.path.join(self.path, item[3]),cv2.IMREAD_COLOR )\n",
    "\n",
    "            batch.append([img1, item[1], item[2], img2, item[4]])\n",
    "            files.append((item[0],item[3]))\n",
    "\n",
    "        return batch, files\n",
    "\n",
    "    def push_batch(self, batch, files):\n",
    "       \n",
    "        for index,item in enumerate(batch):\n",
    "            assert item[0].shape == (dim[0], dim[1], 3)\n",
    "            assert (item[1] < len(numtoactions) and item[1] >= 0)\n",
    "            assert isinstance(item[2],int) or isinstance(item[2],float)\n",
    "            assert item[3].shape == (dim[0], dim[1], 3)\n",
    "            assert os.path.isfile(os.path.join(self.path, files[index][0]))\n",
    "            assert os.path.isfile(os.path.join(self.path, files[index][1]))\n",
    "            \n",
    "            self.shufflelist.append([files[index][0], item[1], item[2], files[index][1], item[4]])\n",
    "    \n",
    "        return\n",
    "    \n",
    "    def get_maxi(self):\n",
    "        \n",
    "        maxi = 0\n",
    "        \n",
    "        for item in self.shufflelist:\n",
    "            curr = item[0]\n",
    "            s = re.findall(r'\\d+', curr)[0]\n",
    "            if int(s) > maxi:\n",
    "                maxi = int(s)\n",
    "        \n",
    "        return maxi\n",
    "    \n",
    "    def get_X(self, batch, state):\n",
    "        \n",
    "        assert state == 0 or state == 3 # 0 is currentstate, 3 is future state\n",
    "        \n",
    "        X = [item[state] for item in batch]\n",
    "\n",
    "        X = np.array(X, dtype=np.float32)\n",
    "        \n",
    "        #for im in X:\n",
    "        #    assert im.min() == 0.0\n",
    "        #    assert im.max() == 255.0 or im.max() == 0.0\n",
    "        \n",
    "        X /= 255.0\n",
    "        \n",
    "        return X\n",
    "    \n",
    "\n",
    "    def backprop(self, batch):\n",
    "\n",
    "        rewards_sample = [batch[i][2] for i in range(len(batch))]\n",
    "        action_sample = [batch[i][1] for i in range(len(batch))]\n",
    "        done_sample = tf.convert_to_tensor([float(batch[i][4]) for i in range(len(batch))])\n",
    "\n",
    "        X =  self.get_X(batch, 0)\n",
    "        Xf = self.get_X(batch, 3)\n",
    "        future_rewards = self.model_target.predict(Xf)\n",
    "\n",
    "        updated_q_values = rewards_sample + 0.99 * tf.reduce_max(future_rewards, axis=1)\n",
    "        updated_q_values = updated_q_values * (1 - done_sample) - done_sample*abs(self.PENALTY)\n",
    "\n",
    "    \n",
    "        masks = tf.one_hot(action_sample, 4)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Train the model on the states and updated Q-values\n",
    "            q_values = self.model(X)\n",
    "\n",
    "            # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "            q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "            # Calculate loss between new Q-value and old Q-value\n",
    "            loss = self.loss_function(updated_q_values, q_action)\n",
    "            \n",
    "            #if self.debugcounter % 20 == 0:\n",
    "            #    print(self.debugcounter)\n",
    "            #    print(updated_q_values)\n",
    "            #    print(rewards_sample)\n",
    "            #    print(q_action)\n",
    "            #self.debugcounter += 1\n",
    "\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        \n",
    " \n",
    "    \n",
    "    \n",
    "    def train(self, i, j, term):\n",
    "        \n",
    "        # https://pythonprogramming.net/training-deep-q-learning-dqn-reinforcement-learning-python-tutorial/\n",
    "        \n",
    "        currentstate = \"current_{}_{}.png\".format(i,j)\n",
    "\n",
    "        nextstate = \"next_{}_{}.png\".format(i,j)      \n",
    "        \n",
    "        batch, files = self.pop_batch(self.BATCHSIZE)\n",
    "        \n",
    "        assert(self.imgresh1.shape == (dim[0], dim[1],3))\n",
    "        assert(self.imgresh2.shape == (dim[0], dim[1],3))\n",
    "        \n",
    "        batch.append([self.imgresh1, actionstonum[self.changeto], self.reward, self.imgresh2, term, self.snake_pos[0], self.snake_pos[1], self.food_pos[0], self.food_pos[1]])\n",
    "        files.append((\"current_{}_{}.png\".format(i,j), \"next_{}_{}.png\".format(i,j)))\n",
    "        \n",
    "        self.write(i,j)\n",
    "         \n",
    "        self.backprop(batch)\n",
    "        \n",
    "        self.numbatches += 1\n",
    "            \n",
    "        self.push_batch(batch, files)   \n",
    "  \n",
    "        return\n",
    "    \n",
    "    def save_checkpoint(self, checkpointparname=modelweightname):\n",
    "                                                                         \n",
    "        self.model_target.set_weights(self.model.get_weights())\n",
    "        print(f\"saving checkpoint: {os.path.join(pathname, modeldirname,checkpointparname)}\")\n",
    "        self.model_target.save_weights(os.path.join(pathname, modeldirname,checkpointparname) )\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def reduce_learning_rate_on_plateau(self, thresh):\n",
    "        if self.checkpoint_counter > thresh:\n",
    "            self.learningrate *= 0.5\n",
    "            opt = Adam(lr=self.learningrate)\n",
    "            self.model.compile(loss=\"mean_squared_error\",optimizer=opt, metrics=['accuracy'])\n",
    "            self.checkpoint_counter = 0\n",
    "            print(f\"learning rate reduced to {self.learningrate} on plateau\")\n",
    "            \n",
    "        return self.learningrate\n",
    "        \n",
    "    def print_benchmark(self):\n",
    "\n",
    "        maxlist = []\n",
    "        penaltylist = []\n",
    "        averagestates = [0,0,0,0]\n",
    "        averagepenalty = [0,0,0,0]\n",
    "        pmerror = 0\n",
    "        pterror = 0\n",
    "\n",
    "        for (cs, act, rew, fs, term) in self.shufflelist:\n",
    "            if rew == self.MAXREWARD or rew == 30.0:\n",
    "                maxlist.append((cs,act,rew,fs,term))\n",
    "            if rew == self.PENALTY:\n",
    "                penaltylist.append((cs,act,rew,fs,term))\n",
    "        print(f\"Number of maxrewards in shufflelist: {len(maxlist)}, perc: {100*len(maxlist)/len(self.shufflelist)}\")\n",
    "        print(f\"Number of terminations in shufflelist: {len(penaltylist)}, perc: {100*len(penaltylist)/len(self.shufflelist)}\")\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        print(\"Testing maxlist\")\n",
    "        for i in range(len(maxlist)):\n",
    "            img = cv2.imread(os.path.join(pathname, datadirname, maxlist[i][0]),cv2.IMREAD_COLOR )\n",
    "            states = self.model.predict(np.array([img])/255.0, batch_size=1, verbose=0)[0]\n",
    "            averagestates += states\n",
    "            if np.argmax(states) != maxlist[i][1]:\n",
    "                count += 1\n",
    "            pmerror = 100*count/len(maxlist)\n",
    "        print(f\"Number of predicted errors in maxlist: {count}, perc: {pmerror}\")\n",
    "        print(f\"Q Values for max: {averagestates/len(maxlist)}\")\n",
    "        \n",
    "        count = 0\n",
    "        \n",
    "        print(\"Testing penaltylist\") \n",
    "        for i in range(len(penaltylist)):\n",
    "            img = cv2.imread(os.path.join(pathname, datadirname, penaltylist[i][0]),cv2.IMREAD_COLOR )\n",
    "            states = self.model.predict(np.array([img])/255.0, batch_size=1, verbose=0)[0]\n",
    "            averagepenalty += states\n",
    "            if np.argmax(states) == penaltylist[i][1]:\n",
    "                count += 1\n",
    "            pterror = 100*count/len(penaltylist)\n",
    "        print(f\"Number of predicted terminations in penaltylist: {count}, perc: {pterror}\")\n",
    "        print(f\"Q Values for penalty: {averagepenalty/len(penaltylist)}\")\n",
    "        \n",
    "        return pmerror, averagestates/len(maxlist), averagepenalty/len(penaltylist)\n",
    "    \n",
    "    def print_score(self):\n",
    "        print(f\" ----> TIME IS {datetime.now():%Y-%m-%d_%H-%M-%S}\")\n",
    "        print(f\" ----> SCORE is {self.score}\")\n",
    "        print(f\" ----> NUM OF BATCHES is {self.numbatches}\")\n",
    "        return self.score, self.numbatches\n",
    "    \n",
    "    def print_overall_score(self):\n",
    "        print(f\"--> TIME IS {datetime.now():%Y-%m-%d_%H-%M-%S}\")\n",
    "        print(f\"--> OVERALL SCORE is {self.overall_score}\")\n",
    "        print(f\"--> OVERALL NUM OF BATCHES is {self.overall_numbatches}\")\n",
    "        return self.overall_score, self.overall_numbatches     \n",
    "    \n",
    "    \n",
    "    def run_replay_memory(self, epochs = 5):\n",
    "        self.model.load_weights(self.checkpointname)\n",
    "        self.load_replay_memory()\n",
    "        for j in range(epochs):\n",
    "            \n",
    "            for i in range(int(len(self.shufflelist)//(self.BATCHSIZE+1))):\n",
    "                if i%500 == 0:\n",
    "                    print(i)\n",
    "                batch, files = self.pop_batch(self.BATCHSIZE+1)\n",
    "                self.backprop(batch)\n",
    "                self.push_batch(batch,files)\n",
    "\n",
    "            self.print_benchmark()\n",
    "            self.save_checkpoint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "overallscores = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(learning_rate = 1.5e-06, epochs = 5, benchmin = 68.0):\n",
    "    manual = False\n",
    "    lr = [learning_rate for i in range(epochs)]\n",
    "    iterations = len(lr)\n",
    "    benches = []\n",
    "    qms = []\n",
    "    qps = []\n",
    "    counter = 0\n",
    "\n",
    "    for i in range(iterations):\n",
    "        print(f\"{i}: learning rate: {lr[i]}\")\n",
    "        print(benchmin)\n",
    "        game = Game(lr[i], \"model-regr.h5\")\n",
    "        k = 150 #40\n",
    "        game.load_replay_memory()\n",
    "        for j in range(k):\n",
    "            game.initialize(i, j)\n",
    "            game.run(j)\n",
    "            #game.reduce_learning_rate_on_plateau(3)\n",
    "            #score = game.print_score()\n",
    "            #scores.append(score)\n",
    "        bench, qm, qp = game.print_benchmark()\n",
    "        benches.append(bench)\n",
    "        qms.append(qm)\n",
    "        qps.append(qp)\n",
    "        game.save_replay_memory()\n",
    "        game.save_checkpoint(f\"model-regr_{i}_{lr[i]:.9f}_{bench:.2f}.h5\")\n",
    "        if bench < benchmin:\n",
    "            benchmin = bench\n",
    "            game.save_checkpoint()\n",
    "        else:\n",
    "            counter += 1\n",
    "        if counter == 3:\n",
    "            counter = 0\n",
    "            lr = 7.5e-07 \n",
    "            \n",
    "        overallscore = game.print_overall_score()\n",
    "        overallscores.append(overallscore)\n",
    "    return benches, qms, qps\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_finder():\n",
    "    manual = False\n",
    "    benchmin = 42.0\n",
    "    lr = [0.0002*pow(0.5,i) for i in range(1,13)]\n",
    "    #lr = [1.5625e-06 for i in range(5)]\n",
    "\n",
    "    iterations = len(lr)\n",
    "    benches = []\n",
    "    qms = []\n",
    "    qps = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        print(f\"{i}: learning rate: {lr[i]}\")\n",
    "        game = Game(lr[i], \"model-regr.h5\")\n",
    "        k = 200 #40\n",
    "        game.load_replay_memory()\n",
    "        for j in range(k):\n",
    "            game.initialize(i, j)\n",
    "            game.run(j)\n",
    "            #game.reduce_learning_rate_on_plateau(3)\n",
    "            #score = game.print_score()\n",
    "            #scores.append(score)\n",
    "        bench, qm, qp = game.print_benchmark()\n",
    "        benches.append(bench)\n",
    "        qms.append(qm)\n",
    "        qps.append(qp)\n",
    "        game.save_replay_memory()\n",
    "        game.save_checkpoint(f\"model-regr_{i}_{lr[i]:.9f}_{bench:.2f}.h5\")\n",
    "        if bench < benchmin:\n",
    "            benchmin = bench\n",
    "            game.save_checkpoint()\n",
    "        overallscore = game.print_overall_score()\n",
    "        overallscores.append(overallscore)\n",
    "    return benches, qms, qps\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: learning rate: 1.5e-06\n",
      "60.0\n",
      "loading checkpoint: C:/Users/Caspar/Desktop/Reinforcement Learning\\model\\model-regr.h5\n",
      "Number of maxrewards in shufflelist: 113, perc: 4.917319408181027\n",
      "Number of terminations in shufflelist: 150, perc: 6.527415143603133\n",
      "Testing maxlist\n",
      "Number of predicted errors in maxlist: 51, perc: 45.13274336283186\n",
      "Q Values for max: [-0.00321075 -0.00197487  0.00169034  0.00411089]\n",
      "Testing penaltylist\n",
      "Number of predicted terminations in penaltylist: 139, perc: 92.66666666666667\n",
      "Q Values for penalty: [-0.00182494 -0.00188731  0.00087587  0.0021519 ]\n",
      "saving checkpoint: C:/Users/Caspar/Desktop/Reinforcement Learning\\model\\model-regr_0_0.000001500_45.13.h5\n",
      "saving checkpoint: C:/Users/Caspar/Desktop/Reinforcement Learning\\model\\model-regr.h5\n",
      "--> TIME IS 2020-11-03_11-21-32\n",
      "--> OVERALL SCORE is 113\n",
      "--> OVERALL NUM OF BATCHES is 0\n",
      "1: learning rate: 1.5e-06\n",
      "45.13274336283186\n",
      "loading checkpoint: C:/Users/Caspar/Desktop/Reinforcement Learning\\model\\model-regr.h5\n",
      "Number of maxrewards in shufflelist: 206, perc: 5.539123420274267\n",
      "Number of terminations in shufflelist: 300, perc: 8.066684592632429\n",
      "Testing maxlist\n",
      "Number of predicted errors in maxlist: 91, perc: 44.1747572815534\n",
      "Q Values for max: [-0.00419488 -0.0016348   0.00139714  0.00380202]\n",
      "Testing penaltylist\n",
      "Number of predicted terminations in penaltylist: 282, perc: 94.0\n",
      "Q Values for penalty: [-0.00167219 -0.00185122  0.00115111  0.0021949 ]\n",
      "saving checkpoint: C:/Users/Caspar/Desktop/Reinforcement Learning\\model\\model-regr_1_0.000001500_44.17.h5\n",
      "saving checkpoint: C:/Users/Caspar/Desktop/Reinforcement Learning\\model\\model-regr.h5\n",
      "--> TIME IS 2020-11-03_11-25-30\n",
      "--> OVERALL SCORE is 93\n",
      "--> OVERALL NUM OF BATCHES is 0\n",
      "2: learning rate: 1.5e-06\n",
      "44.1747572815534\n",
      "loading checkpoint: C:/Users/Caspar/Desktop/Reinforcement Learning\\model\\model-regr.h5\n",
      "Number of maxrewards in shufflelist: 342, perc: 6.268328445747801\n",
      "Number of terminations in shufflelist: 450, perc: 8.247800586510264\n",
      "Testing maxlist\n",
      "Number of predicted errors in maxlist: 167, perc: 48.83040935672515\n",
      "Q Values for max: [-0.00409128 -0.00171954  0.00095217  0.00361172]\n",
      "Testing penaltylist\n",
      "Number of predicted terminations in penaltylist: 415, perc: 92.22222222222223\n",
      "Q Values for penalty: [-0.00171254 -0.00178802  0.0008657   0.00208333]\n",
      "saving checkpoint: C:/Users/Caspar/Desktop/Reinforcement Learning\\model\\model-regr_2_0.000001500_48.83.h5\n",
      "--> TIME IS 2020-11-03_11-30-05\n",
      "--> OVERALL SCORE is 136\n",
      "--> OVERALL NUM OF BATCHES is 0\n",
      "3: learning rate: 1.5e-06\n",
      "44.1747572815534\n",
      "loading checkpoint: C:/Users/Caspar/Desktop/Reinforcement Learning\\model\\model-regr.h5\n",
      "Number of maxrewards in shufflelist: 458, perc: 6.5456624267543235\n",
      "Number of terminations in shufflelist: 600, perc: 8.575103615835358\n",
      "Testing maxlist\n",
      "Number of predicted errors in maxlist: 230, perc: 50.21834061135371\n",
      "Q Values for max: [-0.00353988 -0.00193291  0.00104163  0.00348583]\n",
      "Testing penaltylist\n",
      "Number of predicted terminations in penaltylist: 534, perc: 89.0\n",
      "Q Values for penalty: [-0.00179654 -0.00176107  0.00085846  0.00188534]\n",
      "saving checkpoint: C:/Users/Caspar/Desktop/Reinforcement Learning\\model\\model-regr_3_0.000001500_50.22.h5\n",
      "--> TIME IS 2020-11-03_11-34-34\n",
      "--> OVERALL SCORE is 117\n",
      "--> OVERALL NUM OF BATCHES is 0\n",
      "4: learning rate: 1.5e-06\n",
      "44.1747572815534\n",
      "loading checkpoint: C:/Users/Caspar/Desktop/Reinforcement Learning\\model\\model-regr.h5\n",
      "Number of maxrewards in shufflelist: 567, perc: 6.697377746279234\n",
      "Number of terminations in shufflelist: 750, perc: 8.85896527285613\n",
      "Testing maxlist\n",
      "Number of predicted errors in maxlist: 258, perc: 45.5026455026455\n",
      "Q Values for max: [-0.0037774  -0.00196488  0.00107157  0.00367984]\n",
      "Testing penaltylist\n",
      "Number of predicted terminations in penaltylist: 713, perc: 95.06666666666666\n",
      "Q Values for penalty: [-0.00190141 -0.00202185  0.00108928  0.00225554]\n",
      "saving checkpoint: C:/Users/Caspar/Desktop/Reinforcement Learning\\model\\model-regr_4_0.000001500_45.50.h5\n",
      "--> TIME IS 2020-11-03_11-39-03\n",
      "--> OVERALL SCORE is 111\n",
      "--> OVERALL NUM OF BATCHES is 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([45.13274336283186,\n",
       "  44.1747572815534,\n",
       "  48.83040935672515,\n",
       "  50.21834061135371,\n",
       "  45.5026455026455],\n",
       " [array([-0.00321075, -0.00197487,  0.00169034,  0.00411089]),\n",
       "  array([-0.00419488, -0.0016348 ,  0.00139714,  0.00380202]),\n",
       "  array([-0.00409128, -0.00171954,  0.00095217,  0.00361172]),\n",
       "  array([-0.00353988, -0.00193291,  0.00104163,  0.00348583]),\n",
       "  array([-0.0037774 , -0.00196488,  0.00107157,  0.00367984])],\n",
       " [array([-0.00182494, -0.00188731,  0.00087587,  0.0021519 ]),\n",
       "  array([-0.00167219, -0.00185122,  0.00115111,  0.0021949 ]),\n",
       "  array([-0.00171254, -0.00178802,  0.0008657 ,  0.00208333]),\n",
       "  array([-0.00179654, -0.00176107,  0.00085846,  0.00188534]),\n",
       "  array([-0.00190141, -0.00202185,  0.00108928,  0.00225554])])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_game(1.5e-06, 5, 60.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game = Game(6.0e-07, \"model-regr.h5\")\n",
    "#game.run_replay_memory(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#game.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-168-55603964b8fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxscale\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'log'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbenches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'red'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lr' is not defined"
     ]
    }
   ],
   "source": [
    "rr = lr[::-1]\n",
    "plt.xscale('log')\n",
    "plt.plot(lr,benches, color = 'red')\n",
    "\n",
    "print(lr)\n",
    "#print(rr)\n",
    "print(benches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'game' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-166-c20365c99e14>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#print(scores)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearningrate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprintscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'game' is not defined"
     ]
    }
   ],
   "source": [
    "#print(scores)\n",
    "print(game.learningrate)\n",
    "\n",
    "printscores = []\n",
    "for item in scores:\n",
    "    printscores.append(float(item[0]))\n",
    "\n",
    "printscores_filtered = savgol_filter(printscores, 51, 9)    \n",
    "plt.plot(printscores, color = 'red')\n",
    "plt.plot(printscores_filtered, color = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If mode is 'interp', window_length must be less than or equal to the size of x.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-a6c26fdceb1a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprintscores_filtered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msavgol_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprintscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m21\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprintscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'red'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprintscores_filtered\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'blue'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\signal\\_savitzky_golay.py\u001b[0m in \u001b[0;36msavgol_filter\u001b[1;34m(x, window_length, polyorder, deriv, delta, axis, mode, cval)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"interp\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwindow_length\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m             raise ValueError(\"If mode is 'interp', window_length must be less \"\n\u001b[0m\u001b[0;32m    340\u001b[0m                              \"than or equal to the size of x.\")\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: If mode is 'interp', window_length must be less than or equal to the size of x."
     ]
    }
   ],
   "source": [
    "printscores = []\n",
    "for item in overallscores:\n",
    "    printscores.append(float(item[0])/float(item[1]))\n",
    "\n",
    "    \n",
    "printscores_filtered = savgol_filter(printscores, 21, 9)    \n",
    "plt.plot(printscores, color = 'red')\n",
    "plt.plot(printscores_filtered, color = 'blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'game' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-169-64f20d831925>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_overall_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'game' is not defined"
     ]
    }
   ],
   "source": [
    "game.print_overall_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = np.random.rand(10)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.54428788e-04 7.53220878e-04 8.90904954e-04 5.99854140e-04\n",
      " 2.21794594e-05 1.20994919e-04 6.93502771e-04 2.98423462e-04\n",
      " 6.84108977e-04 4.11687631e-04]\n"
     ]
    }
   ],
   "source": [
    "print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-27_08-19-02\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "\n",
    "print(f\"{datetime.now():%Y-%m-%d_%H-%M-%S}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45455\n"
     ]
    }
   ],
   "source": [
    "print(f\"{0.454545454545:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'game' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-d6274fef2843>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshufflelist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0macount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrew\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshufflelist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'game' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(game.shufflelist))\n",
    "acount = 0\n",
    "pcount = 0\n",
    "mcount = 0\n",
    "for (cs, act, rew, fs, term) in game.shufflelist:\n",
    "    if rew == game.MAXREWARD:\n",
    "        acount += 1\n",
    "    if rew == game.MOVEPENALTY:\n",
    "        mcount += 1\n",
    "    if rew == game.PENALTY:\n",
    "        pcount += 1\n",
    "print(acount)\n",
    "print(pcount)\n",
    "print(mcount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00025511 0.00030566 0.00041123 0.00026548 0.00030797]\n"
     ]
    }
   ],
   "source": [
    "lr = np.array([np.random.uniform(0.5,1) for i in range(5)])/2000\n",
    "print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.001, 0.0005, 0.00025, 0.000125, 6.25e-05, 3.125e-05, 1.5625e-05, 7.8125e-06, 3.90625e-06]\n"
     ]
    }
   ],
   "source": [
    "lr = [0.002*pow(0.5,i) for i in range(1,10)]\n",
    "print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.477225575051661"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/Caspar/Desktop/Reinforcement Learning')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "pathlib.Path().absolute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.5e-07"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.5e-06*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
